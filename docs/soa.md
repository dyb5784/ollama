For 2023, https://www.reddit.com/r/LocalLLaMA/comments/164xiuc/comment/jyau2z4/?utm_source=share&utm_medium=web2x&context=3

In January, Anthropic released Claude v1, a 100 billion parameter LLM that aims to be more interpretable, robust, and aligned with human values. Claude v1 uses a novel architecture called the Causal Transformer, which can model both causal and non-causal dependencies in natural language. Claude v1 also incorporates human feedback and adversarial testing to improve its performance and safety1

In February, AI21 Labs launched Jurassic-1, a 178 billion parameter LLM that can generate high-quality and diverse text on various domains and tasks. Jurassic-1 is trained on a large and diverse corpus of web text, books, news, and scientific articles. Jurassic-1 also supports multiple languages, such as English, Spanish, French, German, and Hebrew1

In March, OpenAI released GPT-4, the largest and most powerful LLM to date, with 100 trillion parameters. GPT-4 is the first multimodal LLM that can accept both text and images as input, and generate coherent and relevant output. GPT-4 can perform many NLP tasks with minimal or no fine-tuning, using prompt engineering. GPT-4 also demonstrates unprecedented levels of natural language understanding and generation, as well as complex reasoning and coding skills2

In April, LG AI Research announced WizardLM, a 50 billion parameter LLM that can generate realistic and engaging dialogues on various topics. WizardLM is trained on a large corpus of human conversations from various sources, such as social media, podcasts, movies, and books. WizardLM can also handle multiple modalities, such as text, speech, and emotion1

In May, Meta AI revealed OPT-IML, a 200 billion parameter LLM that can optimize itself for any given task or domain. OPT-IML uses a meta-learning approach that allows it to learn from its own experience and adapt to new situations. OPT-IML can also generate novel solutions and insights for various problems, such as drug discovery, climate change, and education1

In June, Baidu launched Ernie 3.0 Titan, a 300 billion parameter LLM that can understand and generate natural language across multiple domains and languages. Ernie 3.0 Titan is trained on a massive multilingual corpus of web text, books, news, and encyclopedias. Ernie 3.0 Titan can also perform cross-lingual transfer learning and zero-shot learning for various NLP tasks1

In July, Huawei unveiled PanGu-Alpha, a 600 billion parameter LLM that can generate high-quality and fluent text on various domains and tasks. PanGu-Alpha is trained on a large corpus of Chinese web text, books, news, and scientific articles. PanGu-Alpha can also support multiple languages, such as English, French, German, and Arabic1